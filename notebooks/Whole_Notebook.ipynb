{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522802ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== OPTIONAL INSTALLS (uncomment if needed) =====================\n",
    "# !pip install rasterio --quiet\n",
    "# !pip install fiona --quiet\n",
    "# !pip install geopandas --quiet\n",
    "\n",
    "# ===================== IMPORTS =====================\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.features import shapes as rio_shapes\n",
    "from shapely.geometry import shape, mapping\n",
    "from skimage.morphology import remove_small_objects, opening, closing, disk\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, concatenate, Input, Dropout, MaxPooling2D, Conv2DTranspose\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "# Paths and filenames\n",
    "CKPT_DIR = \"checkpoints\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "BEST_MODEL_PATH = os.path.join(CKPT_DIR, \"best_change_detection_unet_model.keras\")\n",
    "\n",
    "OUT_DIR = \"outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_PROB = os.path.join(OUT_DIR, \"change_prob_improved.tif\")\n",
    "OUT_MASK = os.path.join(OUT_DIR, \"change_mask_improved.tif\")\n",
    "OUT_VECTOR_SHP = os.path.join(OUT_DIR, \"change_polygons.shp\")\n",
    "OUT_VECTOR_GEOJSON = os.path.join(OUT_DIR, \"change_polygons.geojson\")\n",
    "\n",
    "BEFORE_TIF = \"/kaggle/input/testtiff12/processed_img1.tif\"\n",
    "AFTER_TIF = \"/kaggle/input/testtiff12/processed_img2.tif\"\n",
    "\n",
    "# Model & inference parameters\n",
    "INPUT_SHAPE = (256, 256, 6)\n",
    "PATCH_H = 256\n",
    "PATCH_W = 256\n",
    "OVERLAP = 64\n",
    "BATCH_SIZE = 8\n",
    "THRESH = 0.50\n",
    "USE_TTA = True\n",
    "\n",
    "# Postprocessing\n",
    "MIN_CC_SIZE = 256\n",
    "MORPH_OPEN_DISK = 2\n",
    "MORPH_CLOSE_DISK = 3\n",
    "\n",
    "# ===================== METRIC =====================\n",
    "# Name this iou so ModelCheckpoint can monitor \"val_iou\"\n",
    "def iou(y_true, y_pred, smooth=1e-6):\n",
    "    y_pred_bin = tf.cast(y_pred > 0.5, tf.float32)\n",
    "    y_true_f = tf.cast(y_true, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_bin)\n",
    "    union = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_bin) - intersection\n",
    "    return (intersection + smooth) / (union + smooth)\n",
    "\n",
    "# ===================== MODEL BUILD =====================\n",
    "def build_unet_model(input_size=INPUT_SHAPE):\n",
    "    inputs = Input(input_size)\n",
    "    input_image1 = inputs[..., :3]\n",
    "    input_image2 = inputs[..., 3:]\n",
    "\n",
    "    # Encoder 1\n",
    "    c1_1 = Conv2D(64, 3, activation='relu', padding='same')(input_image1)\n",
    "    c1_1 = Conv2D(64, 3, activation='relu', padding='same')(c1_1)\n",
    "    c1_1 = Dropout(0.3)(c1_1)\n",
    "    p1_1 = MaxPooling2D((2, 2))(c1_1)\n",
    "\n",
    "    c2_1 = Conv2D(128, 3, activation='relu', padding='same')(p1_1)\n",
    "    c2_1 = Conv2D(128, 3, activation='relu', padding='same')(c2_1)\n",
    "    c2_1 = Dropout(0.3)(c2_1)\n",
    "    p2_1 = MaxPooling2D((2, 2))(c2_1)\n",
    "\n",
    "    c3_1 = Conv2D(256, 3, activation='relu', padding='same')(p2_1)\n",
    "    c3_1 = Conv2D(256, 3, activation='relu', padding='same')(c3_1)\n",
    "    c3_1 = Dropout(0.3)(c3_1)\n",
    "    p3_1 = MaxPooling2D((2, 2))(c3_1)\n",
    "\n",
    "    # Encoder 2 (same)\n",
    "    c1_2 = Conv2D(64, 3, activation='relu', padding='same')(input_image2)\n",
    "    c1_2 = Conv2D(64, 3, activation='relu', padding='same')(c1_2)\n",
    "    c1_2 = Dropout(0.3)(c1_2)\n",
    "    p1_2 = MaxPooling2D((2, 2))(c1_2)\n",
    "\n",
    "    c2_2 = Conv2D(128, 3, activation='relu', padding='same')(p1_2)\n",
    "    c2_2 = Conv2D(128, 3, activation='relu', padding='same')(c2_2)\n",
    "    c2_2 = Dropout(0.3)(c2_2)\n",
    "    p2_2 = MaxPooling2D((2, 2))(c2_2)\n",
    "\n",
    "    c3_2 = Conv2D(256, 3, activation='relu', padding='same')(p2_2)\n",
    "    c3_2 = Conv2D(256, 3, activation='relu', padding='same')(c3_2)\n",
    "    c3_2 = Dropout(0.3)(c3_2)\n",
    "    p3_2 = MaxPooling2D((2, 2))(c3_2)\n",
    "\n",
    "    # Bottleneck\n",
    "    c4_1 = Conv2D(512, 3, activation='relu', padding='same')(p3_1)\n",
    "    c4_1 = Conv2D(512, 3, activation='relu', padding='same')(c4_1)\n",
    "    c4_1 = Dropout(0.4)(c4_1)\n",
    "\n",
    "    c4_2 = Conv2D(512, 3, activation='relu', padding='same')(p3_2)\n",
    "    c4_2 = Conv2D(512, 3, activation='relu', padding='same')(c4_2)\n",
    "    c4_2 = Dropout(0.4)(c4_2)\n",
    "\n",
    "    c4 = concatenate([c4_1, c4_2])\n",
    "\n",
    "    # Decoder\n",
    "    u5 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c4)\n",
    "    u5 = concatenate([u5, c3_1, c3_2])\n",
    "    c5 = Conv2D(256, 3, activation='relu', padding='same')(u5)\n",
    "    c5 = Dropout(0.3)(c5)\n",
    "    c5 = Conv2D(256, 3, activation='relu', padding='same')(c5)\n",
    "\n",
    "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = concatenate([u6, c2_1, c2_2])\n",
    "    c6 = Conv2D(128, 3, activation='relu', padding='same')(u6)\n",
    "    c6 = Dropout(0.3)(c6)\n",
    "    c6 = Conv2D(128, 3, activation='relu', padding='same')(c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c1_1, c1_2])\n",
    "    c7 = Conv2D(64, 3, activation='relu', padding='same')(u7)\n",
    "    c7 = Dropout(0.3)(c7)\n",
    "    c7 = Conv2D(64, 3, activation='relu', padding='same')(c7)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c7)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Build & compile\n",
    "model_unet = build_unet_model(INPUT_SHAPE)\n",
    "model_unet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=[iou, 'accuracy'])\n",
    "print(\"Model built. Input shape:\", model_unet.input_shape)\n",
    "\n",
    "# ===================== DATA LOADING (LEVIR-CD example) =====================\n",
    "# Adjust these paths to your dataset location if necessary\n",
    "image1_train_dir = \"/kaggle/input/levir-cd/LEVIR CD/train/A\"\n",
    "image2_train_dir = \"/kaggle/input/levir-cd/LEVIR CD/train/B\"\n",
    "mask_train_dir   = \"/kaggle/input/levir-cd/LEVIR CD/train/label\"\n",
    "\n",
    "image1_test_dir = \"/kaggle/input/levir-cd/LEVIR CD/test/A\"\n",
    "image2_test_dir = \"/kaggle/input/levir-cd/LEVIR CD/test/B\"\n",
    "mask_test_dir   = \"/kaggle/input/levir-cd/LEVIR CD/test/label\"\n",
    "\n",
    "RESIZE_SHAPE = (256, 256)\n",
    "\n",
    "def load_images(image1_dir, image2_dir, mask_dir):\n",
    "    files1 = sorted(os.listdir(image1_dir))\n",
    "    files2 = sorted(os.listdir(image2_dir))\n",
    "    filesm = sorted(os.listdir(mask_dir))\n",
    "    X, y = [], []\n",
    "    for f1, f2, fm in zip(files1, files2, filesm):\n",
    "        i1 = cv2.imread(os.path.join(image1_dir, f1))\n",
    "        i2 = cv2.imread(os.path.join(image2_dir, f2))\n",
    "        m  = cv2.imread(os.path.join(mask_dir, fm), cv2.IMREAD_GRAYSCALE)\n",
    "        i1 = cv2.resize(i1, RESIZE_SHAPE)\n",
    "        i2 = cv2.resize(i2, RESIZE_SHAPE)\n",
    "        m  = cv2.resize(m, RESIZE_SHAPE)\n",
    "        i1 = i1.astype(np.float32)/255.0\n",
    "        i2 = i2.astype(np.float32)/255.0\n",
    "        m  = (m.astype(np.float32)/255.0).astype(np.float32)\n",
    "        stacked = np.concatenate([i1, i2], axis=-1)  # (H,W,6)\n",
    "        X.append(stacked)\n",
    "        y.append(m[..., np.newaxis])  # keep channel dim (H,W,1)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load (this may take time)\n",
    "X_all, y_all = load_images(image1_train_dir, image2_train_dir, mask_train_dir)\n",
    "X_test, y_test = load_images(image1_test_dir, image2_test_dir, mask_test_dir)\n",
    "\n",
    "# Train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
    "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "# ===================== CALLBACKS & TRAIN =====================\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(BEST_MODEL_PATH, monitor=\"val_loss\", save_best_only=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
    "]\n",
    "\n",
    "history_unet = model_unet.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Show history keys and checkpoint dir for debugging\n",
    "print(\"history keys:\", list(history_unet.history.keys()))\n",
    "print(\"checkpoint files:\", os.listdir(CKPT_DIR))\n",
    "\n",
    "# ===================== ENSURE & LOAD BEST MODEL =====================\n",
    "def ensure_and_load_best_model(in_memory_model, path=BEST_MODEL_PATH):\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            print(\"Loading saved model from:\", path)\n",
    "            m = tf.keras.models.load_model(path, compile=False)\n",
    "            print(\"Loaded model from checkpoint.\")\n",
    "            return m\n",
    "        except Exception as e:\n",
    "            print(\"Warning: load_model failed:\", e)\n",
    "    # Fallback: save current in-memory model (EarlyStopping restored best weights) then load\n",
    "    print(\"Saving current in-memory model to:\", path)\n",
    "    in_memory_model.save(path, include_optimizer=False)\n",
    "    return tf.keras.models.load_model(path, compile=False)\n",
    "\n",
    "MODEL_UNET_BEST = ensure_and_load_best_model(model_unet, BEST_MODEL_PATH)\n",
    "print(\"MODEL_UNET_BEST input shape:\", MODEL_UNET_BEST.input_shape)\n",
    "\n",
    "# Evaluate using the best model (safer)\n",
    "try:\n",
    "    loss, acc = MODEL_UNET_BEST.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=1)\n",
    "    print(\"Test loss, accuracy (best model):\", loss, acc)\n",
    "except Exception as e:\n",
    "    print(\"Evaluation failed:\", e)\n",
    "\n",
    "# ===================== INFERENCE HELPERS =====================\n",
    "def read_window(src, col_off, row_off, w, h, bands=None):\n",
    "    if bands is None:\n",
    "        bands = src.count\n",
    "    band_indices = list(range(1, bands + 1))\n",
    "    arr = src.read(band_indices, window=Window(col_off, row_off, w, h))\n",
    "    return np.moveaxis(arr, 0, -1)\n",
    "\n",
    "def infer_bands_from_model(model):\n",
    "    try:\n",
    "        inp_shape = model.input_shape\n",
    "        if inp_shape is None or len(inp_shape) != 4:\n",
    "            return None\n",
    "        c = inp_shape[-1]\n",
    "        if c % 2 == 0:\n",
    "            return c // 2\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def gaussian_window(h, w, sigma_scale=0.125):\n",
    "    yy = np.linspace(-1, 1, h)\n",
    "    xx = np.linspace(-1, 1, w)\n",
    "    xv, yv = np.meshgrid(xx, yy)\n",
    "    sigma = sigma_scale\n",
    "    g = np.exp(- (xv**2 + yv**2) / (2 * sigma**2))\n",
    "    g = g / g.max()\n",
    "    return g.astype(np.float32)\n",
    "\n",
    "def apply_tta_predict(model, batch):\n",
    "    preds = []\n",
    "    p0 = model.predict(batch, verbose=0)\n",
    "    p0 = np.squeeze(p0, axis=-1) if (p0.ndim == 4 and p0.shape[-1] == 1) else p0\n",
    "    preds.append(p0)\n",
    "    batch_h = batch[:, :, ::-1, :]\n",
    "    p_h = model.predict(batch_h, verbose=0)\n",
    "    p_h = np.squeeze(p_h, axis=-1) if (p_h.ndim == 4 and p_h.shape[-1] == 1) else p_h\n",
    "    preds.append(p_h[:, :, ::-1])\n",
    "    batch_v = batch[:, ::-1, :, :]\n",
    "    p_v = model.predict(batch_v, verbose=0)\n",
    "    p_v = np.squeeze(p_v, axis=-1) if (p_v.ndim == 4 and p_v.shape[-1] == 1) else p_v\n",
    "    preds.append(p_v[:, ::-1, :])\n",
    "    batch_hv = batch_h[:, ::-1, :, :]\n",
    "    p_hv = model.predict(batch_hv, verbose=0)\n",
    "    p_hv = np.squeeze(p_hv, axis=-1) if (p_hv.ndim == 4 and p_hv.shape[-1] == 1) else p_hv\n",
    "    preds.append(p_hv[:, ::-1, ::-1])\n",
    "    return np.mean(preds, axis=0)\n",
    "\n",
    "# ===================== TILED INFERENCE + VECTORIZE =====================\n",
    "def predict_full_tif_with_model_improved(\n",
    "        model,\n",
    "        before_path,\n",
    "        after_path,\n",
    "        out_prob_path,\n",
    "        out_mask_path,\n",
    "        out_vector_shp_path=None,\n",
    "        out_vector_geojson_path=None,\n",
    "        patch_h=None,\n",
    "        patch_w=None,\n",
    "        overlap=64,\n",
    "        threshold=0.65,\n",
    "        batch_size=8,\n",
    "        use_tta=True,\n",
    "        min_cc_size=256,\n",
    "        morph_open_disk=2,\n",
    "        morph_close_disk=3\n",
    "    ):\n",
    "    if getattr(model, \"input_shape\", None) is None:\n",
    "        raise RuntimeError(\"Model has no input_shape.\")\n",
    "    inp_shape = model.input_shape\n",
    "    model_H, model_W, model_C = inp_shape[1], inp_shape[2], inp_shape[3]\n",
    "    if patch_h is None: patch_h = model_H\n",
    "    if patch_w is None: patch_w = model_W\n",
    "\n",
    "    inferred_bands = infer_bands_from_model(model)\n",
    "\n",
    "    if out_vector_shp_path is None:\n",
    "        out_vector_shp_path = OUT_VECTOR_SHP\n",
    "    if out_vector_geojson_path is None:\n",
    "        out_vector_geojson_path = OUT_VECTOR_GEOJSON\n",
    "\n",
    "    with rasterio.open(before_path) as sb, rasterio.open(after_path) as sa:\n",
    "        assert sb.width == sa.width and sb.height == sa.height, \"Before/After dims mismatch\"\n",
    "        width, height = sb.width, sb.height\n",
    "        profile = sb.profile.copy()\n",
    "        profile.update(count=1, dtype=\"float32\", compress=\"lzw\")\n",
    "\n",
    "        prob_sum = np.zeros((height, width), dtype=np.float32)\n",
    "        weight_sum = np.zeros((height, width), dtype=np.float32)\n",
    "\n",
    "        step_x = patch_w - overlap\n",
    "        step_y = patch_h - overlap\n",
    "        xs = list(range(0, max(1, width - overlap), step_x))\n",
    "        ys = list(range(0, max(1, height - overlap), step_y))\n",
    "        if xs[-1] + patch_w < width:\n",
    "            xs.append(max(0, width - patch_w))\n",
    "        if ys[-1] + patch_h < height:\n",
    "            ys.append(max(0, height - patch_h))\n",
    "        tiles = [(x, y) for y in ys for x in xs]\n",
    "\n",
    "        gw = gaussian_window(patch_h, patch_w)\n",
    "\n",
    "        print(f\"üß© Raster: {width}x{height}, tiles: {len(tiles)}, patch: {patch_h}x{patch_w}, overlap: {overlap}\")\n",
    "\n",
    "        batches, batch_meta = [], []\n",
    "\n",
    "        for (x_off, y_off) in tqdm(tiles, desc=\"Processing tiles\"):\n",
    "            w = min(patch_w, width - x_off)\n",
    "            h = min(patch_h, height - y_off)\n",
    "\n",
    "            b_patch = read_window(sb, x_off, y_off, w, h, bands=inferred_bands or sb.count)\n",
    "            a_patch = read_window(sa, x_off, y_off, w, h, bands=inferred_bands or sa.count)\n",
    "            if b_patch.shape[-1] != a_patch.shape[-1]:\n",
    "                raise RuntimeError(f\"Band mismatch: {b_patch.shape[-1]} vs {a_patch.shape[-1]}\")\n",
    "\n",
    "            pad_w, pad_h = (patch_w - w), (patch_h - h)\n",
    "            if pad_w or pad_h:\n",
    "                b_patch = np.pad(b_patch, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n",
    "                a_patch = np.pad(a_patch, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n",
    "\n",
    "            inp = np.concatenate([b_patch, a_patch], axis=-1).astype(np.float32) / 255.0\n",
    "            if inp.shape[-1] != model.input_shape[-1]:\n",
    "                raise RuntimeError(f\"Expected {model.input_shape[-1]} channels, got {inp.shape[-1]}.\")\n",
    "\n",
    "            batches.append(inp)\n",
    "            batch_meta.append((x_off, y_off, h, w))\n",
    "\n",
    "            if len(batches) >= batch_size:\n",
    "                batch_arr = np.stack(batches, axis=0)\n",
    "                preds = apply_tta_predict(model, batch_arr) if use_tta else np.squeeze(model.predict(batch_arr, verbose=0), axis=-1)\n",
    "                for i, (x_i, y_i, h_i, w_i) in enumerate(batch_meta):\n",
    "                    p = preds[i][:h_i, :w_i]\n",
    "                    gw_crop = gw[:h_i, :w_i]\n",
    "                    prob_sum[y_i:y_i+h_i, x_i:x_i+w_i] += (p * gw_crop)\n",
    "                    weight_sum[y_i:y_i+h_i, x_i:x_i+w_i] += gw_crop\n",
    "                batches, batch_meta = [], []\n",
    "\n",
    "        # leftover\n",
    "        if batches:\n",
    "            batch_arr = np.stack(batches, axis=0)\n",
    "            preds = apply_tta_predict(model, batch_arr) if use_tta else np.squeeze(model.predict(batch_arr, verbose=0), axis=-1)\n",
    "            for i, (x_i, y_i, h_i, w_i) in enumerate(batch_meta):\n",
    "                p = preds[i][:h_i, :w_i]\n",
    "                gw_crop = gw[:h_i, :w_i]\n",
    "                prob_sum[y_i:y_i+h_i, x_i:x_i+w_i] += (p * gw_crop)\n",
    "                weight_sum[y_i:y_i+h_i, x_i:x_i+w_i] += gw_crop\n",
    "\n",
    "        # normalize (avoid division by zero)\n",
    "        final_prob = np.zeros_like(prob_sum, dtype=np.float32)\n",
    "        nonzero = weight_sum > 0\n",
    "        final_prob[nonzero] = prob_sum[nonzero] / weight_sum[nonzero]\n",
    "\n",
    "        # write probability raster\n",
    "        with rasterio.open(out_prob_path, \"w\", **profile) as dst:\n",
    "            dst.write(final_prob.astype(np.float32), 1)\n",
    "\n",
    "        # threshold + morphology + small objects removal\n",
    "        mask = (final_prob > threshold).astype(bool)\n",
    "        if morph_open_disk > 0: mask = opening(mask, disk(morph_open_disk))\n",
    "        if morph_close_disk > 0: mask = closing(mask, disk(morph_close_disk))\n",
    "        if min_cc_size > 0: mask = remove_small_objects(mask, min_size=min_cc_size)\n",
    "        mask_final = (mask.astype(np.uint8) * 255)\n",
    "\n",
    "        mask_profile = profile.copy()\n",
    "        mask_profile.update(dtype=\"uint8\", count=1)\n",
    "        with rasterio.open(out_mask_path, \"w\", **mask_profile) as dst:\n",
    "            dst.write(mask_final, 1)\n",
    "\n",
    "        # Vectorize mask\n",
    "        features = []\n",
    "        transform = sb.transform\n",
    "        src_crs = sb.crs\n",
    "        bin_mask = (mask_final != 0).astype(np.uint8)\n",
    "\n",
    "        for geom, val in rio_shapes(bin_mask, transform=transform):\n",
    "            if int(val) == 0: \n",
    "                continue\n",
    "            geom_shp = shape(geom)\n",
    "            if geom_shp.is_valid and not geom_shp.is_empty:\n",
    "                pix_w = abs(transform.a) if hasattr(transform, \"a\") else None\n",
    "                pix_h = abs(transform.e) if hasattr(transform, \"e\") else None\n",
    "                pixel_area = (pix_w * pix_h) if (pix_w is not None and pix_h is not None) else None\n",
    "                pixel_count = int(round(geom_shp.area / pixel_area)) if pixel_area and pixel_area > 0 else -1\n",
    "                geom_area = float(geom_shp.area)\n",
    "                features.append({\n",
    "                    \"type\": \"Feature\",\n",
    "                    \"geometry\": mapping(geom_shp),\n",
    "                    \"properties\": {\"value\": int(val), \"pixel_count\": pixel_count, \"area_map_units\": geom_area}\n",
    "                })\n",
    "\n",
    "        shp_written = None\n",
    "        geojson_written_path = None\n",
    "\n",
    "        # Try writing Shapefile via geopandas\n",
    "        try:\n",
    "            import geopandas as gpd\n",
    "            geojson_fc = {\"type\": \"FeatureCollection\", \"features\": features}\n",
    "\n",
    "            crs_for_gdf = None\n",
    "            prj_wkt = None\n",
    "            if src_crs is not None:\n",
    "                try:\n",
    "                    crs_for_gdf = src_crs.to_dict()\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        prj_wkt = src_crs.to_wkt() if hasattr(src_crs, \"to_wkt\") else None\n",
    "                        crs_for_gdf = src_crs.to_string() if hasattr(src_crs, \"to_string\") else None\n",
    "                    except Exception:\n",
    "                        crs_for_gdf = None\n",
    "                        prj_wkt = None\n",
    "\n",
    "            gdf = gpd.GeoDataFrame.from_features(geojson_fc, crs=crs_for_gdf)\n",
    "            shp_dir = os.path.dirname(out_vector_shp_path)\n",
    "            if shp_dir and not os.path.isdir(shp_dir):\n",
    "                os.makedirs(shp_dir, exist_ok=True)\n",
    "            gdf.to_file(out_vector_shp_path, driver=\"ESRI Shapefile\", index=False)\n",
    "            shp_written = out_vector_shp_path\n",
    "\n",
    "            # Write .prj if we have WKT\n",
    "            try:\n",
    "                if prj_wkt is None and src_crs is not None and hasattr(src_crs, \"to_wkt\"):\n",
    "                    prj_wkt = src_crs.to_wkt()\n",
    "                if prj_wkt:\n",
    "                    prj_path = os.path.splitext(out_vector_shp_path)[0] + \".prj\"\n",
    "                    with open(prj_path, \"w\", encoding=\"utf-8\") as pf:\n",
    "                        pf.write(prj_wkt)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è geopandas->shapefile attempt failed: {e}\\nFalling back to GeoJSON at: {out_vector_geojson_path}.\")\n",
    "            try:\n",
    "                geojson_fc = {\"type\": \"FeatureCollection\", \"features\": features}\n",
    "                with open(out_vector_geojson_path, \"w\", encoding=\"utf-8\") as jf:\n",
    "                    json.dump(geojson_fc, jf, ensure_ascii=False, indent=2)\n",
    "                geojson_written_path = out_vector_geojson_path\n",
    "                if src_crs is not None and hasattr(src_crs, \"to_wkt\"):\n",
    "                    try:\n",
    "                        prj_path = os.path.splitext(out_vector_geojson_path)[0] + \".prj\"\n",
    "                        with open(prj_path, \"w\", encoding=\"utf-8\") as pf:\n",
    "                            pf.write(src_crs.to_wkt())\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            except Exception as e2:\n",
    "                print(f\"‚ö†Ô∏è Also failed to write fallback GeoJSON: {e2}\")\n",
    "\n",
    "        # Final prints & return\n",
    "        print(f\"‚úÖ Done. Saved:\\n  - Prob map: {out_prob_path}\\n  - Mask: {out_mask_path}\")\n",
    "        if shp_written:\n",
    "            print(f\"  - Vector (Shapefile): {shp_written}\")\n",
    "        elif geojson_written_path:\n",
    "            print(f\"  - Vector (GeoJSON fallback): {geojson_written_path}\")\n",
    "        else:\n",
    "            print(\"  - Vector: none (failed to write shapefile or geojson)\")\n",
    "\n",
    "        return out_prob_path, out_mask_path, shp_written, geojson_written_path\n",
    "\n",
    "# ===================== RUN INFERENCE (use the loaded best model) =====================\n",
    "out_prob, out_mask, out_shp, out_geojson = predict_full_tif_with_model_improved(\n",
    "    MODEL_UNET_BEST,\n",
    "    BEFORE_TIF,\n",
    "    AFTER_TIF,\n",
    "    out_prob_path=OUT_PROB,\n",
    "    out_mask_path=OUT_MASK,\n",
    "    out_vector_shp_path=OUT_VECTOR_SHP,\n",
    "    out_vector_geojson_path=OUT_VECTOR_GEOJSON,\n",
    "    patch_h=PATCH_H,\n",
    "    patch_w=PATCH_W,\n",
    "    overlap=OVERLAP,\n",
    "    threshold=THRESH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    use_tta=USE_TTA,\n",
    "    min_cc_size=MIN_CC_SIZE,\n",
    "    morph_open_disk=MORPH_OPEN_DISK,\n",
    "    morph_close_disk=MORPH_CLOSE_DISK\n",
    ")\n",
    "\n",
    "print(\"\\nReturn values:\")\n",
    "print(\"  out_prob:\", out_prob)\n",
    "print(\"  out_mask:\", out_mask)\n",
    "print(\"  out_shp:\", out_shp)\n",
    "print(\"  out_geojson:\", out_geojson)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
